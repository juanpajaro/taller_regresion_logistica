# Taller de Regresi√≥n Log√≠stica - PLN
**Maestr√≠a en Inteligencia Artificial - Universidad Javeriana 2025**

## üìã Descripci√≥n del Taller

Este taller implementa un clasificador de sentimientos para tweets utilizando **Regresi√≥n Log√≠stica** desde cero. Los estudiantes aprender√°n a procesar texto, extraer caracter√≠sticas y entrenar un modelo de machine learning para an√°lisis de sentimientos.

## üéØ Objetivos de Aprendizaje

Al completar este taller, los estudiantes ser√°n capaces de:

1. **Preprocesar texto** eliminando ruido y normalizando tweets
2. **Extraer caracter√≠sticas** relevantes para an√°lisis de sentimientos  
3. **Implementar regresi√≥n log√≠stica** desde cero usando NumPy
4. **Evaluar modelos** de clasificaci√≥n binaria
5. **Aplicar t√©cnicas de PLN** en problemas reales

## üõ†Ô∏è Requisitos Previos

### Conocimientos:
- Python intermedio
- Conceptos b√°sicos de machine learning
- √Ålgebra lineal b√°sica
- Procesamiento de lenguaje natural

### Librer√≠as requeridas:
```bash
pip install nltk numpy
```

## üìö Funciones a Implementar

### 1. **Funci√≥n `limpiar_tweet(tweet)`**

**Objetivo:** Preprocesar tweets eliminando ruido y normalizando el texto.

**Tareas a completar:**
```python
def limpiar_tweet(tweet):
    # COMPLETAR:
    stemmer = ???  # Inicializar PorterStemmer
    stop_words = ???  # Cargar stopwords en ingl√©s
    
    # Usar expresiones regulares para:
    tweet = ???  # Eliminar s√≠mbolos de monetizaci√≥n ($100, $USD)
    tweet = ???  # Eliminar URLs (https://...)
    tweet = ???  # Eliminar menciones (@usuario)
    tweet = ???  # Eliminar caracteres especiales y n√∫meros
```

**Pistas importantes:**
- Use `PorterStemmer()` de NLTK
- Use `stopwords.words('english')` para obtener stopwords
- Expresiones regulares √∫tiles:
  - `r'\$\w*'` para s√≠mbolos monetarios
  - `r'https?://[^\s\n\r]+'` para URLs
  - `r'@\w+'` para menciones
  - `r'[^a-zA-Z\s]'` para caracteres no alfab√©ticos

**Resultado esperado:**
```
Input: "I love this movie! @user #happy https://example.com"
Output: ['love', 'movi', 'happi']
```

### 2. **Funci√≥n `construir_frecuencias(tweets, etiquetas)`**

**Objetivo:** Construir un diccionario de frecuencias de palabras por sentimiento.

**Tareas a completar:**
```python
def construir_frecuencias(tweets, etiquetas):
    # COMPLETAR:
    if pair in frecuencias:
        frecuencias[pair] += ???  # Incrementar contador
    else:
        frecuencias[pair] = ???  # Inicializar contador
```

**Explicaci√≥n:**
- `pair = (sentimiento, palabra)` donde sentimiento es 0 (negativo) o 1 (positivo)
- El diccionario mapea `(sentimiento, palabra) ‚Üí frecuencia`

**Resultado esperado:**
```
{(1, 'love'): 4, (0, 'hate'): 3, (1, 'great'): 2, ...}
```

### 3. **Funci√≥n `sigmoid(z)` - ¬°FUNCI√ìN CLAVE!**

**Objetivo:** Implementar la funci√≥n sigmoide para regresi√≥n log√≠stica.

**Tarea a completar:**
```python
def sigmoid(z):
    # COMPLETAR:
    h = ???  # F√≥rmula: 1 / (1 + e^(-z))
```

**F√≥rmula matem√°tica:**
```
œÉ(z) = 1 / (1 + e^(-z))
```

**Pistas:**
- Use `np.exp()` para la exponencial
- Maneje casos donde z es muy grande o peque√±o

**Resultado esperado:**
```
Input: [-2, -1, 0, 1, 2]
Output: [0.1192, 0.2689, 0.5, 0.7311, 0.8808]
```

### 4. **Funci√≥n `gradiente_descendente(X, y, theta, alpha, num_iter)`**

**Objetivo:** Optimizar los par√°metros del modelo usando gradiente descendente.

**Tareas a completar:**
```python
def gradiente_descendente(X, y, theta, alpha, num_iter):
    # COMPLETAR:
    m = ???  # N√∫mero de ejemplos de entrenamiento
    
    for i in range(0, num_iter):
        z = ???        # Producto punto X * theta
        h = ???        # Aplicar sigmoid a z
        J = ???        # Calcular funci√≥n de costo (log-likelihood)
        theta = ???    # Actualizar par√°metros
```

**F√≥rmulas matem√°ticas:**
- **Predicci√≥n:** `z = X @ theta`
- **Probabilidad:** `h = sigmoid(z)`
- **Costo:** `J = -(1/m) * [y.T @ log(h) + (1-y).T @ log(1-h)]`
- **Gradiente:** `theta = theta - (alpha/m) * X.T @ (h-y)`

**Resultado esperado:**
```
Costo final: ~0.32
Theta final: [-0.067, 1.106, 1.429]
```

### 5. **Funci√≥n `test_logistic_regression(test_x, test_y, frecuencias, theta)`**

**Objetivo:** Evaluar el rendimiento del modelo entrenado.

**Tarea a completar:**
```python
def test_logistic_regression(test_x, test_y, frecuencias, theta):
    # COMPLETAR:
    accuracy = ???  # Calcular exactitud: predicciones_correctas / total
```

**Pista:**
- Compare `y_hat` (predicciones) con `test_y` (etiquetas reales)
- Use `np.mean()` para calcular el porcentaje de aciertos

## üß™ Pruebas y Validaci√≥n

El repositorio incluye archivos de prueba para validar su implementaci√≥n:

- `test_sigmoid.py` - Valida la funci√≥n sigmoide
- `test_trinos_limpios.py` - Valida la limpieza de tweets
- `test_construir_frec.py` - Valida la construcci√≥n de frecuencias
- `test_gradiente.py` - Valida el gradiente descendente
- `test_evaluacion_rl.py` - Valida la evaluaci√≥n del modelo

**Para ejecutar las pruebas:**
```bash
python3 test_sigmoid.py
python3 test_trinos_limpios.py
python3 test_construir_frec.py
python3 test_gradiente.py
python3 test_evaluacion_rl.py
```

## üìä Resultados Esperados

Al ejecutar `python3 taller.py`, debe obtener resultados similares a:

```
N√∫mero de tweets positivos: 5000
N√∫mero de tweets negativos: 5000
N√∫mero de palabras √∫nicas en el vocabulario: ~9000
Probabilidades sigmoid: [0.6225 0.3775 0.8175]
Costo despu√©s del entrenamiento: 0.24921235
Exactitud del modelo: 0.9950
```

## üéì Criterios de Evaluaci√≥n

| Funci√≥n | Puntos | Criterios |
|---------|--------|-----------|
| `limpiar_tweet` | 10 | Elimina correctamente URLs, menciones, caracteres especiales |
| `construir_frecuencias` | 10 | Construye diccionario correcto de frecuencias |
| `sigmoid` | 10 | Implementa correctamente la funci√≥n sigmoide |
| `gradiente_descendente` | 10 | Optimiza par√°metros correctamente |
| `test_logistic_regression` | 10 | Calcula exactitud correctamente |
| **Total** | **100** | |


## üìñ Conceptos Te√≥ricos Clave

### Regresi√≥n Log√≠stica:
- **Clasificaci√≥n binaria** usando funci√≥n sigmoide
- **Maximizaci√≥n de verosimilitud** como objetivo
- **Gradiente descendente** para optimizaci√≥n

### Procesamiento de Texto:
- **Tokenizaci√≥n** para dividir texto en palabras
- **Stemming** para normalizar variaciones morfol√≥gicas
- **Stopwords** para eliminar palabras comunes sin significado
- **Bag of Words** como representaci√≥n de caracter√≠sticas

### Evaluaci√≥n:
- **Exactitud (Accuracy)** como m√©trica principal
- **Matriz de confusi√≥n** para an√°lisis detallado
- **Validaci√≥n** en conjunto de prueba independiente

## üöÄ Entrega

1. **Archivo principal:** `taller.py` con todas las funciones implementadas
2. **Pruebas:** Todos los archivos `test_*.py` deben pasar exitosamente

**Fecha de entrega:** 25 agosto 2025, 6 pm
**Formato:** Repositorio Git con c√≥digo fuente completo

---

## üìû Soporte

Para dudas acad√©micas, contactar:
- **Profesor:** Juan Pajaro
- **Email:** juanpajaro@javeriana.edu.co
- **Horarios de atenci√≥n:** Martes 5 pm

**¬°Buena suerte con el taller! üéâ**
